{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOYzfMRPjgbFkXtjxnVozlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aldrin-Fanir/Hippocampal-Region-Segmentation-UNet/blob/main/HippocampalRegionUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Library"
      ],
      "metadata": {
        "id": "VuWYDv-bY0DT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_TzYjYYam4"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "from math import atan2, cos, sin, sqrt, pi, log\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from numpy import linalg as LA\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drive Mount: Import Google Drive"
      ],
      "metadata": {
        "id": "sthynycXo3TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76cIp12FpYIq",
        "outputId": "aac41f7c-8c45-4d27-9ea8-1f9b0c25baf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directory of Images and Masks"
      ],
      "metadata": {
        "id": "JPxW3IZupu3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/MyDrive/HippocampalRegionSegmentationUNet/Cohort1-Multiplexed Dataset'"
      ],
      "metadata": {
        "id": "4LSSt1EVpuZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Is path correct?\", os.path.exists(root_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGjckpbaqKEJ",
        "outputId": "3188ac72-f3ef-47dc-d42d-c9ea78485a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is path correct? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hippocampal Region Dataset"
      ],
      "metadata": {
        "id": "PcbLw-OcqlVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HippocampalDataset(Dataset):\n",
        "  def __init__(self, root_path, limit = None):\n",
        "    self.root_path = root_path\n",
        "    self.limit = limit\n",
        "\n",
        "    self.images = sorted([root_path + \"/cFos_NeuN_dFos_dataset_images/\" + i for i in os.listdir(root_path + \"/cFos_NeuN_dFos_dataset_images\")])[: self.limit]\n",
        "    self.masks = sorted([root_path + \"/cFos_NeuN_dFos_dataset_masks/\" + i for i in os.listdir(root_path + \"/cFos_NeuN_dFos_dataset_masks\")])[: self.limit]\n",
        "\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    if self.limit is None:\n",
        "      self.limit = len(self.images)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "    mask = Image.open(self.masks[index]).convert(\"L\")\n",
        "\n",
        "    return self.transform(img), self.transform(mask)\n",
        "\n",
        "  def __len__(self):\n",
        "    return min(len(self.images), self.limit)\n"
      ],
      "metadata": {
        "id": "b10tFIWsqsNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Dataset"
      ],
      "metadata": {
        "id": "JwUfOzXcsz_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = HippocampalDataset(root_path, limit = None)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size = 1, shuffle = True)\n",
        "\n",
        "for i, (images, masks) in enumerate(loader):\n",
        "  print(f'Batch {i+1}')\n",
        "  print(f'Image Shape: {images.shape}')\n",
        "  print(f'Mask Shape: {masks.shape}')\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  #Train Image\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(images[0].permute(1, 2, 0).cpu().numpy())\n",
        "  plt.title(f'Original Image {i+1}')\n",
        "  plt.axis('off')\n",
        "\n",
        "  #Train Mask\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(masks[0].permute(1, 2, 0).cpu().numpy())\n",
        "  plt.title(f'Mask Image {i+1}')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bVJ49jNBs3gk",
        "outputId": "01929779-cde4-4ede-f7d0-5cc687721cc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "kh_qxfe51Q-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double Convolution"
      ],
      "metadata": {
        "id": "YpnE7R3h1Tku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv_op = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(inplace = True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv_op(x)"
      ],
      "metadata": {
        "id": "oCnrsh_O1YO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downsampling"
      ],
      "metadata": {
        "id": "3fDxpMGW0N5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv = DoubleConv(in_channels, out_channels)\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    down = self.conv(x)\n",
        "    p = self.pool(down)\n",
        "\n",
        "    return down, p"
      ],
      "metadata": {
        "id": "6WOZAEXp2eL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upsampling"
      ],
      "metadata": {
        "id": "qH0QMjp93Tbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size = 2, stride = 2)\n",
        "    self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "    x = torch.cat([x1, x2], 1)\n",
        "\n",
        "    return self.conv(x)"
      ],
      "metadata": {
        "id": "lXcmm6pG3WCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Architecture"
      ],
      "metadata": {
        "id": "5VmVgbDB4pYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.down_convolution_1 = DownSample(in_channels, 64)\n",
        "    self.down_convolution_2 = DownSample(64, 128)\n",
        "    self.down_convolution_3 = DownSample(128, 256)\n",
        "    self.down_convolution_4 = DownSample(256, 512)\n",
        "\n",
        "    self.bottle_neck = DoubleConv(512, 1024)\n",
        "\n",
        "    self.up_convolution_1 = UpSample(1024, 512)\n",
        "    self.up_convolution_2 = UpSample(512, 256)\n",
        "    self.up_convolution_3 = UpSample(256, 128)\n",
        "    self.up_convolution_4 = UpSample(128, 64)\n",
        "\n",
        "    self.output = nn.Conv2d(64, out_channels = num_classes, kernel_size = 1)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    down_1, p1 = self.down_convolution_1(x)\n",
        "    down_2, p2 = self.down_convolution_2(p1)\n",
        "    down_3, p3 = self.down_convolution_3(p2)\n",
        "    down_4, p4 = self.down_convolution_4(p3)\n",
        "\n",
        "    b = self.bottle_neck(p4)\n",
        "\n",
        "    up_1 = self.up_convolution_1(b, down_4)\n",
        "    up_2 = self.up_convolution_2(up_1, down_3)\n",
        "    up_3 = self.up_convolution_3(up_2, down_2)\n",
        "    up_4 = self.up_convolution_4(up_3, down_1)\n",
        "\n",
        "    return self.output(up_4)"
      ],
      "metadata": {
        "id": "Ku6Dt-Jn4rwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainning The Model"
      ],
      "metadata": {
        "id": "LBeJoUmC6JLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = HippocampalDataset(root_path)\n",
        "generator = torch.Generator().manual_seed(42)"
      ],
      "metadata": {
        "id": "p6rgsf1N6L5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = random_split(train_dataset, [0.8, 0.2], generator = generator)"
      ],
      "metadata": {
        "id": "EZgnBHMQ6W3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset, val_dataset = random_split(test_dataset, [0.5, 0.5], generator = generator)"
      ],
      "metadata": {
        "id": "ZREnWwJC6djw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running this experiment with CUDA**"
      ],
      "metadata": {
        "id": "ekbsDJ6K6tAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if device == \"cuda\":\n",
        "  num_workers = torch.cuda.device_count()*4"
      ],
      "metadata": {
        "id": "TqNUB7YY6xMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now We setup out model using the AdamW optimizer and the BCEWithLogitsLoss**"
      ],
      "metadata": {
        "id": "LOeFhaoJ7MXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Learning_Rate = 3e-4\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(dataset = train_dataset, num_workers = num_workers, pin_memory = True, batch_size = batch_size, shuffle = True)\n",
        "test_dataloader = DataLoader(dataset = test_dataset, num_workers = num_workers, pin_memory = True, batch_size = batch_size, shuffle = False)\n",
        "val_dataloader = DataLoader(dataset = val_dataset, num_workers = num_workers, pin_memory = True, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "model = UNet(in_channels = 3, num_classes = 1).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr = Learning_Rate)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "skzf326Q7U13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghm-ujTOHVf_",
        "outputId": "03a3c12d-8754-49a2-8f04-2983847a2434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "239\n",
            "59\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Segmentation Performance with DICEMetric**"
      ],
      "metadata": {
        "id": "9LK-Kdfl9sn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coefficient(prediction, target, epsilon = 1e-07):\n",
        "  prediction_copy = prediction.clone()\n",
        "\n",
        "  prediction_copy[prediction_copy<0] = 0\n",
        "  prediction_copy[prediction_copy>0] = 1\n",
        "\n",
        "  intersection = abs(torch.sum(prediction_copy * target))\n",
        "  union = torch.sum(prediction_copy) + torch.sum(target)\n",
        "  dice = (2.0 * intersection + epsilon) / (union + epsilon)\n",
        "\n",
        "  return dice"
      ],
      "metadata": {
        "id": "6NmN6sks9ypm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KfLhiOFu-cTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "EkeHM2sT-kro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train_losses = []\n",
        "train_dcs = []\n",
        "\n",
        "val_losses = []\n",
        "val_dcs = []\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  model.train()\n",
        "  train_running_loss = 0\n",
        "  traing_running_dc = 0\n",
        "\n",
        "  for idx, img_mask in enumerate(tqdm(train_dataloader, position = 0, leave=True)):\n",
        "    img = img_mask[0].float().to(device)\n",
        "    mask = img_mask[1].float().to(device)\n",
        "\n",
        "    y_pred = model(img)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    dc = dice_coefficient(y_pred, mask)\n",
        "    loss = criterion(y_pred, mask)\n",
        "\n",
        "    train_running_loss += loss.item()\n",
        "    traing_running_dc += dc.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_running_loss/(idx + 1)\n",
        "  train_dc = traing_running_dc/(idx + 1)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  train_dcs.append(train_dc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  val_running_loss = 0\n",
        "  val_running_dc = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, img_mask in enumerate(tqdm(val_dataloader, position=0, leave = True)):\n",
        "      img = img_mask[0].float().to(device)\n",
        "      mask = img_mask[1].float().to(device)\n",
        "\n",
        "      y_pred = model(img)\n",
        "      loss = criterion(y_pred , mask)\n",
        "      dc = dice_coefficient(y_pred, mask)\n",
        "\n",
        "      val_running_loss += loss.item()\n",
        "      val_running_dc += dc.item()\n",
        "\n",
        "    val_loss = val_running_loss / (idx + 1)\n",
        "    val_dc = val_running_dc / (idx + 1)\n",
        "\n",
        "  val_losses.append(val_loss)\n",
        "  val_dcs.append(val_dc)\n",
        "\n",
        "\n",
        "\n",
        "  print(\"-\" * 30)\n",
        "  print(f\"Training Loss EPOCH {epoch + 1}: {train_loss:.4f}\")\n",
        "  print(f\"Training DICE EPOCH {epoch + 1}: {train_dc:.4f}\")\n",
        "  print(\"\\n\")\n",
        "  print(f\"Validation Loss EPOCH {epoch + 1}: {val_loss:.4f}\")\n",
        "  print(f\"Validation DICE EPOCH {epoch + 1}: {val_dc:.4f}\")\n",
        "  print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab6f-ar6-oOz",
        "outputId": "88c4cb0d-3acb-42a3-b917-ba378592f59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.37it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n",
            " 10%|█         | 1/10 [00:05<00:49,  5.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 1: 0.5652\n",
            "Training DICE EPOCH 1: 0.0103\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 1: 0.3763\n",
            "Validation DICE EPOCH 1: 0.0000\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.86it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n",
            " 20%|██        | 2/10 [00:10<00:41,  5.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 2: 0.3274\n",
            "Training DICE EPOCH 2: 0.0000\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 2: 0.2535\n",
            "Validation DICE EPOCH 2: 0.0000\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.72it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
            " 30%|███       | 3/10 [00:15<00:36,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 3: 0.2325\n",
            "Training DICE EPOCH 3: 0.0966\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 3: 0.2099\n",
            "Validation DICE EPOCH 3: 0.5405\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.82it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n",
            " 40%|████      | 4/10 [00:20<00:30,  5.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 4: 0.2202\n",
            "Training DICE EPOCH 4: 0.4504\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 4: 0.1976\n",
            "Validation DICE EPOCH 4: 0.4808\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.87it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n",
            " 50%|█████     | 5/10 [00:25<00:25,  5.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 5: 0.2102\n",
            "Training DICE EPOCH 5: 0.4326\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 5: 0.1924\n",
            "Validation DICE EPOCH 5: 0.5543\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.73it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n",
            " 60%|██████    | 6/10 [00:30<00:20,  5.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 6: 0.2045\n",
            "Training DICE EPOCH 6: 0.5198\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 6: 0.1911\n",
            "Validation DICE EPOCH 6: 0.5879\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.87it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n",
            " 70%|███████   | 7/10 [00:35<00:14,  5.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 7: 0.2010\n",
            "Training DICE EPOCH 7: 0.5363\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 7: 0.1782\n",
            "Validation DICE EPOCH 7: 0.6066\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.78it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.93it/s]\n",
            " 80%|████████  | 8/10 [00:40<00:10,  5.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 8: 0.2051\n",
            "Training DICE EPOCH 8: 0.5289\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 8: 0.1845\n",
            "Validation DICE EPOCH 8: 0.5808\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.83it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.90it/s]\n",
            " 90%|█████████ | 9/10 [00:45<00:04,  5.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 9: 0.2063\n",
            "Training DICE EPOCH 9: 0.5070\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 9: 0.1900\n",
            "Validation DICE EPOCH 9: 0.5962\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.79it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n",
            "100%|██████████| 10/10 [00:50<00:00,  5.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Training Loss EPOCH 10: 0.2017\n",
            "Training DICE EPOCH 10: 0.5222\n",
            "\n",
            "\n",
            "Validation Loss EPOCH 10: 0.1892\n",
            "Validation DICE EPOCH 10: 0.5931\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}